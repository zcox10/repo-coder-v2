{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd460d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.tools import Tools\n",
    "from src.utils.constants import Constants\n",
    "from src.utils.compute_score import ComputeScore\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8876a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = ComputeScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc5b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_benchmark: str = \"random_api\"\n",
    "line_benchmark: str = \"random_line\"\n",
    "\n",
    "# Shorter variants for CodeGen\n",
    "short_api_benchmark: str = \"short_api\"\n",
    "short_line_benchmark: str = \"short_line\"\n",
    "\n",
    "final_scores = {\n",
    "    \"benchmark\": \"random_api\",\n",
    "    \"repos\": {\n",
    "        \"repo_1\": [],\n",
    "        \"repo_2\": [],\n",
    "    },\n",
    "    \"final_scores\": {\n",
    "        \"total_samples\": 0,\n",
    "        \"mean_em_score\": 0,\n",
    "        \"mean_es_score\": 0,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05852793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '    unet = UNet2DConditionModel.from_pretrained(\\n        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\\n    )'}]\n",
      "[{'text': '        if self.device != torch.device(\"meta\") or not hasattr(self.unet, \"_hf_hook\"):'}]\n",
      "[{'text': '        pipeline = pipeline.to(accelerator.device)'}]\n",
      "[{'text': '                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()'}]\n",
      "[{'text': '                self.numpy_to_pil(image), return_tensors=\"np\"'}]\n",
      "[{'text': '            obj2 = SampleObject.load_config(tmpdirname)'}]\n",
      "[{'text': '                text_input_ids.to(device),'}]\n",
      "[{'text': '                t = t.to(\"cpu\")'}]\n",
      "[{'text': '    unet.to(accelerator.device, dtype=weight_dtype)'}]\n",
      "[{'text': '            self.safety_checker.check_image(image, device, dtype)'}]\n"
     ]
    }
   ],
   "source": [
    "lines = Tools.load_jsonl(\n",
    "    \"data/cache/predictions/short_api/r-g-one-gram-ws-20-ss-2.codegen-350M-mono.jsonl\"\n",
    ")\n",
    "passk = 1\n",
    "\n",
    "scores = {}\n",
    "for line in lines:\n",
    "    repo = line[\"metadata\"][\"task_id\"].split(\"/\")[0]\n",
    "    samples = [line[\"choices\"][i][\"text\"] for i in range(len(line[\"choices\"]))][:passk]\n",
    "    ground_truth = line[\"metadata\"][\"ground_truth\"]\n",
    "\n",
    "    print(line[\"choices\"])\n",
    "    # print(line[\"metadata\"].keys())\n",
    "\n",
    "#     em_score = score.compute_EM(ground_truth, samples)\n",
    "#     es_score = score.compute_ES(ground_truth, samples)\n",
    "\n",
    "#     # Initialize list for repo if it doesn't exist\n",
    "#     if repo not in scores:\n",
    "#         scores[repo] = {\n",
    "#             \"total_samples\": None,\n",
    "#             \"summed_em_score\": None,\n",
    "#             \"mean_em_score\": None,\n",
    "#             \"summed_es_score\": None,\n",
    "#             \"mean_es_score\": None,\n",
    "#             \"samples\": [],\n",
    "#         }\n",
    "\n",
    "#     scores[repo][\"samples\"].append(\n",
    "#         {\n",
    "#             \"ground_truth\": ground_truth,\n",
    "#             \"samples\": samples,\n",
    "#             \"em_score\": em_score,\n",
    "#             \"es_score\": es_score,\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "# for repo, score_dict in scores.items():\n",
    "#     total_samples = 0\n",
    "#     summed_em_score = 0\n",
    "#     mean_em_score = 0.0\n",
    "#     summed_es_score = 0.0\n",
    "#     mean_es_score = 0.0\n",
    "#     for sample in score_dict[\"samples\"]:\n",
    "#         total_samples += 1\n",
    "#         summed_em_score += sample[\"em_score\"]\n",
    "#         summed_es_score += sample[\"es_score\"]\n",
    "\n",
    "#     score_dict[\"total_samples\"] = total_samples\n",
    "#     score_dict[\"summed_em_score\"] = summed_em_score\n",
    "#     score_dict[\"summed_es_score\"] = summed_es_score\n",
    "#     score_dict[\"mean_em_score\"] = summed_em_score / total_samples\n",
    "#     score_dict[\"mean_es_score\"] = summed_es_score / total_samples\n",
    "\n",
    "# pprint(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = Tools.load_jsonl(Constants.short_api_completion_benchmark)\n",
    "\n",
    "for task in tasks[0:1]:\n",
    "    print(f\"\\n########## prompt ##########\\n{task['prompt']}\")\n",
    "\n",
    "    for metadata_key, metadata_value in task[\"metadata\"].items():\n",
    "        print(f\"\\n########## metadata.{metadata_key} ##########\\n{metadata_value}\")\n",
    "        # print(f\"metadata.{metadata_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval = Tools.load_pickle(\n",
    "    \"data/cache/retrieval/short_api/gt/huggingface_diffusers_ws20_slice2.huggingface_diffusers_ws20_slice2.one-gram.top10.pkl\"\n",
    ")\n",
    "\n",
    "for line in retrieval[:5]:\n",
    "    # pprint(line)\n",
    "    print(f\"\\n#################### Task Context ####################\\n{line['context']}\")\n",
    "\n",
    "    for metadata_key, metadata_value in task[\"metadata\"].items():\n",
    "        print(f\"\\n########## task.metadata.{metadata_key} ##########\\n{metadata_value}\")\n",
    "\n",
    "    for data in line[\"data\"]:\n",
    "        print(f\"\\n########## Task Embedding Data ##########\\n{data}\")\n",
    "\n",
    "    for top_k, data in enumerate(line[\"top_k_context\"], 1):\n",
    "        print(f\"\\n########## Top K {top_k} ##########\")\n",
    "        print(f\"Similarity Score: {data[1]}\")\n",
    "\n",
    "        print(f\"metadata.context (code lines): {len(data[0]['context'].splitlines())}\")\n",
    "\n",
    "        for i in range(len(data[0][\"metadata\"])):\n",
    "\n",
    "            for mkey, mval in data[0][\"metadata\"][i].items():\n",
    "                print(f\"metadata.{mkey}: {mval}\")\n",
    "            print()\n",
    "\n",
    "        for dkey, dval in data[0][\"data\"][0].items():\n",
    "            print(f\"data.{dkey}: {len(dval)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9569609",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_gt = Tools.load_jsonl(\"data/cache/prompts/gt-one-gram-ws-20-ss-2.jsonl\")\n",
    "prompts_rg = Tools.load_jsonl(\"data/cache/prompts/r-g-one-gram-ws-20-ss-2.jsonl\")\n",
    "\n",
    "for prompt in prompts_rg[:1]:\n",
    "    print(\"\\nGROUND TRUTH:\")\n",
    "    print(prompt[\"metadata\"][\"ground_truth\"])\n",
    "\n",
    "    print(\"\\nPROMPT:\")\n",
    "    print(prompt[\"prompt\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repo_coder_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
